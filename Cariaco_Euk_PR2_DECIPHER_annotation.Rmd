---
title: "Annotation of Cariaco Eukaryotic 18S Amplicons using App in Cyverse Discovery Environment"
author: "Liz Suter"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---
<br>

# Prepping and getting the data ready

First steps are to use `iinit` in terminal to connect to Data Store so you can access the fastq files
Directions [here](https://learning.cyverse.org/projects/data_store_guide/en/latest/step2.html#icommands-first-time-configuration)

In summary, use the following when prompted:  
Host name (DNS) of the server to connect to: data.cyverse.org  
Port number: 1247  
irods user name  
irods zone: iplant  
iRODS password  



### Install and load packages

No need to do any installations in VICE app. Packages should come pre-installed. But need to run libraries:

```{r}
library(dada2)
library(DECIPHER)
```


### Get sample names- run in Terminal
```bash
cd raw_data
ls *_1.fastq.gz | cut -f 1 -d "_" > ../samples
```

### Take a look at the untrimmed reads
```{r}
## import sample names as R variable
samples <- scan("samples", what="character")

# make variable holding the file names of all the forward read fastq files. These are in a subdirectory, so I am also adding the name of the sub directory to the file name
forward_reads <- paste0("raw_data/", samples, "_1.fastq.gz")
# and one with the reverse
reverse_reads <- paste0("raw_data/", samples, "_2.fastq.gz")

# And plot using a tool from dada2 (checking only 5 samples for plotting purposes)
plotQualityProfile(forward_reads[1:5])
plotQualityProfile(reverse_reads[1:5])
```

From the above you can see the reads are ~300bp and the quality is OK, with the R reads being poorer than the F reads. 

### Removing primers using cutadapt- run in Terminal
Sequencing facility told me that the way that they sequence in a way that primers do not get sequenced. Still, check for and remove possible primers


Primers from [Stoeck et al. 2010](https://doi.org/10.1111/j.1365-294X.2009.04480.x):
TAReuk454FWD1: (5'- CCAGCASCYGCGGTAATTCC -3')  
TAReukREV3 (5'- ACTTTCGTTCTTGATYRA -3')

(Also found a usefule [website](https://github.com/pr2database/pr2-primers/wiki/18S-rRNA-primer-sets) with euk primers from PR2 people: 

Cut F primer and rev comp of R primer from F reads with -a option
Cut R primer and rev comp of F primer from R reads with -A option

Set the min size. I tried setting it to be about 10% smaller than the amplicon length (~260) but after playing around and even going down to 200bp, it was throwing out most reads. Alternatively, I set NO min length but then it was writing empty reads to the fast1 file, which led to errors afterward. So decided on min length of 150 to compromise.

```bash
cd ..
mkdir trimmed_fastq
cd raw_data

# Run in loop

for sample in $(cat ../samples)
do
    echo "On sample: $sample"
cutadapt -a ^CCAGCASCYGCGGTAATTCC...TYRATCAAGAACGAAAGT \
-A ^ACTTTCGTTCTTGATYRA...GGAATTACCGCRGSTGCTGG \
--discard-untrimmed \
-m 150 \
-o ../trimmed_fastq/${sample}_1_trimmed.fastq.gz -p ../trimmed_fastq/${sample}_2_trimmed.fastq.gz \
${sample}_1.fastq.gz ${sample}_2.fastq.gz \
>> ../trimmed_fastq/cutadapt_primer_trimming_stats.txt 2>&1
done
```


Check output stats

```bash
paste ../samples <(grep "passing" ../trimmed_fastq/cutadapt_primer_trimming_stats.txt | cut -f3 -d "(" | tr -d ")") <(grep "filtered" ../trimmed_fastq/cutadapt_primer_trimming_stats.txt | cut -f3 -d "(" | tr -d ")")
```

Retained ~30-90% of reads in most cases 

# DADA2- back to R console

### Set our working directory and list our files
```{r}
setwd("~/")
list.files() # make sure what we think is here is actually here
```

Now let's take a look at the trimmed reads.

```{r}
forward_reads_trimmed <- paste0("trimmed_fastq/", samples, "_1_trimmed.fastq.gz")
reverse_reads_trimmed <- paste0("trimmed_fastq/", samples, "_2_trimmed.fastq.gz")

# And plot 
plotQualityProfile(forward_reads_trimmed[1:5])
plotQualityProfile(reverse_reads_trimmed[1:5])
```

Comparing the above to the pre-trimmed reads, they look very similar but got rid of short messiness in beginning of reads (primer). Can't see much change in end of reads but I know they were trimmed based on output file. Overall quality is too low to tell.

### Quality Filtering
Make a directory for filtered reads
```bash
cd ..
mkdir filtered_fastq
```

Make variables containing the file names for the new filtered forward and reverse reads that we will make
```{r}
filtered_forward_reads <- paste0("filtered_fastq/",samples, "_1_filtered.fastq.gz")
filtered_reverse_reads <- paste0("filtered_fastq/",samples, "_2_filtered.fastq.gz")
```


Based on how the quality plots look, determine how much to cut from each side. Trim the F reads at 275 Trim the R reads to 200 Also I want to run this step to trim out the low-quality individual reads (set maxEE to 1 for both F and R reads). The rm.phix = TRUE option removes any leftover  PhiX genomic DNA (that gets added as a standard during sequencing). Pick a min length ~shorter than the min trimmed length (in this case 140 for R reads). I also set truncQ to truncate any read that has a quality score less than 2. Multithreading for this function does not work well (even according to documentation) so needed to skip that. Takes a while to run
```{r}
filtered_out <- filterAndTrim(forward_reads_trimmed, filtered_forward_reads,
                reverse_reads_trimmed, filtered_reverse_reads, maxEE=c(2,2),
                rm.phix=TRUE, minLen=140, truncLen=c(275,200), truncQ = 2, maxN=0)
```

Check out the quality profiles again.
```{r}
filtered_out

plotQualityProfile(filtered_forward_reads[1:5])
plotQualityProfile(filtered_reverse_reads[1:5])
```
Look much better

Save workspace up to this point.
```{r}
save.image(file = "Cariaco_euk18S_dada2_upto_filterfastq.RData")
```

And back up in data store just in case
```bash
iput -f Cariaco_euk18S_dada2_upto_filterfastq.RData
iput -f Cariaco_Euk_PR2_DECIPHER_annotation.Rmd
iput -f samples
iput -rf filtered_fastq
iput -rf trimmed_fastq
```

```{r}
# load("Cariaco_euk18S_dada2_upto_filterfastq.RData")
```


### Error profiling 
Next, DADA2 tries to learn the error signature of our dataset. This step takes a while
```{r}
err_forward_reads <- learnErrors(filtered_forward_reads, multithread=TRUE)
err_reverse_reads <- learnErrors(filtered_reverse_reads, multithread=TRUE)
```



Plot the error profiles
```{r}
plotErrors(err_forward_reads, nominalQ=TRUE)
plotErrors(err_reverse_reads, nominalQ=TRUE)
```

The creators of DADA2 describe this [here](https://benjjneb.github.io/dada2/tutorial.html#learn-the-error-rates). The profiles are the error rates for each possible transition in the read (A->C, A->G, etc). Generally in the above plots, you want to see that the black dots (observed error rates for each quality score) match well with the black lines (the estimated error rate). The red line is what is expected based on the quality score.

Backup again since this step above takes awhile
Save workspace up to this point.
```{r}
save.image(file = "Cariaco_euk18S_dada2_upto_errorprofile.RData")
```

And back up in data store just in case
```bash
iput -f Cariaco_euk18S_dada2_upto_errorprofile.RData
iput -f Cariaco_Euk_PR2_DECIPHER_annotation.Rmd
```


### Inferring ASVs 
Use the dada command to infer ASVs. We are going to use the pooling option "psuedo" which is described [here](https://benjjneb.github.io/dada2/pseudo.html#Pseudo-pooling). This step also takes awhile
```{r}
#setwd("20200710backup/") # only used this because had to resetup analysis
dada_forward <- dada(filtered_forward_reads, err=err_forward_reads, pool="pseudo", multithread=TRUE) 
dada_reverse <- dada(filtered_reverse_reads, err=err_reverse_reads, pool="pseudo", multithread=TRUE)
# and save in this code block incase I get logged out again!
save.image(file = "Cariaco_euk18S_dada2_upto_inferasv.RData")
```


And back up in data store just in case
```{bash}
iput Cariaco_euk18S_dada2_upto_inferasv.RData
iput -f Cariaco_Euk_PR2_DECIPHER_annotation.Rmd
```

### Merge inferred reads
Dada2 will merge reads wherever the overlap is identical between the F and R reads. I trimmed the F reads to 275 and R reads to 200 . The full amplicon size (after removing primers) should be 963-583 = 380. So the F read should be sequenced from position ~583 to ~858 (583+275) and the R reads should be sequenced from ~ position 963 to 763 (963-200). This leaves overlap between position ~763 to 858 or about 95bp. Set minimum to about half this


```{r}
#setwd("20200710backup/") # only used this because had to resetup analysis
merged_amplicons <- mergePairs(dada_forward, filtered_forward_reads, dada_reverse, filtered_reverse_reads, trimOverhang=TRUE, minOverlap=50, verbose = TRUE)
```

Back up again
```{r}
save.image(file = "Cariaco_euk18S_upto_merge.RData")
```

```bash
iput HRE_sewage_microbiome_upto_merge.RData
iput -f HRE_Sewage_microbiome_DECIPHER_20200710.Rmd
```

Load from Data Store
```{r}
load("eukaryote_amplicons/Cariaco_euk18S_upto_merge.RData")
```



```{r}
names(merged_amplicons)
# Initially these names have the full name with `fastq.gz` in the name. Change to just sample name
names(merged_amplicons) <- samples

# Check some other things
length(merged_amplicons) # 47 elements in this list, one for each of our samples
class(merged_amplicons$SRR3735256) # each element of the list is a dataframe that can be accessed and manipulated like any ordinary dataframe
names(merged_amplicons$SRR3735256) # the names() function on a dataframe gives you the column names
# "sequence"  "abundance" "forward"   "reverse"   "nmatch"    "nmismatch" "nindel"    "prefer"    "accept"
```


### Creating a sequence table
```{r}
seqtab <- makeSequenceTable(merged_amplicons)
class(seqtab) # matrix
dim(seqtab) # 47 samples, 78182 unique ASVs
```

Backup notebook
```{bash}
iput -f Cariaco_Euk_PR2_DECIPHER_annotation_20200827.Rmd
```


### Removing chimeras
```{r}
seqtab.nochim <- removeBimeraDenovo(seqtab, verbose=TRUE, multithread = TRUE) 

# Identified 3511 bimeras out of 31475 input sequences.

# though we a lot of unique sequences, we don't know if they held a lot in terms of abundance, this is one quick way to look at that
sum(seqtab.nochim)/sum(seqtab) 
# 0.8092498
# lost about 20% which were chimeras (seems high)
```

Backup again since this step above takes awhile
```{r}
save.image(file = "Cariaco_euk18S_upto_chimera.RData")
```

```{bash}
iput -f Cariaco_Euk_PR2_DECIPHER_annotation_20200827_b.Rmd
iput Cariaco_euk18S_upto_chimera.RData
```

```{r}
load("eukaryote_amplicons/Cariaco_euk18S_upto_chimera.RData")
```


# Summary of read counts through the pipeline
```{r}
# set a little function
getN <- function(x) sum(getUniques(x))

# making a little table
summary_tab <- data.frame(row.names=samples, dada2_input=filtered_out[,1],
                          filtered=filtered_out[,2], dada_f=sapply(dada_forward, getN),
                          dada_r=sapply(dada_reverse, getN), merged=sapply(merged_amplicons, getN),
                          nonchim=rowSums(seqtab.nochim),
                          final_perc_reads_retained=round(rowSums(seqtab.nochim)/filtered_out[,1]*100, 1))

summary_tab
```
Retained abot 20-40% of input reads. Most were lost after quality filtering.

### Assigning taxonomy using DECIPHER

Use the PR2 database (version 4.12.0) from [here](https://github.com/pr2database/pr2database/releases) as input fasta for the AssignTaxonomy function. I put this in Data Store (instead of downloading here in the notebook because didn't have github tools installed here). Syntax needs some small changes compared to using Silva databases. Dada2 developers describe those [here](https://benjjneb.github.io/dada2/training.html).

```{r}
taxa <- assignTaxonomy(seqtab.nochim, "eukaryote_amplicons/pr2_version_4.12.0_18S_dada2.fasta.gz", taxLevels = c("Kingdom","Supergroup","Division","Class","Order","Family","Genus","Species"), multithread=TRUE)
```

Check
```{r}
taxa.print <- taxa # Removing sequence rownames for display only
rownames(taxa.print) <- NULL
head(taxa.print)
```


### Extract the files 

```{r}
#giving our seq headers more manageable names (ASV_1, ASV_2...)
asv_seqs <- colnames(seqtab.nochim)
asv_headers <- vector(dim(seqtab.nochim)[2], mode="character")
for (i in 1:dim(seqtab.nochim)[2]) {
  asv_headers[i] <- paste(">ASV", i, sep="_")
}

# making and writing out a fasta of our final ASV seqs:
asv_fasta <- c(rbind(asv_headers, asv_seqs))
write(asv_fasta, "ASVs.fa")

# count table:
asv_tab <- t(seqtab.nochim)
row.names(asv_tab) <- sub(">", "", asv_headers)
write.table(asv_tab, "ASVs_counts.tsv", sep="\t", quote=F, col.names=NA)

# Taxonomy table
asv_tax <- taxa
row.names(asv_tax) <- sub(">", "", asv_headers)
write.table(asv_tax, "ASVs_taxonomy.tsv", sep="\t", quote=F, col.names=NA)
```

Backup and save to Data Store
Save workspace up to this point.
```{r}
save.image(file = "Cariaco_euk18S_final.RData")
```

And back up in data store just in case
```bash
iput Cariaco_euk18S_final.RData
iput -f Cariaco_Euk_PR2_DECIPHER_annotation.Rmd
iput ASVs.fa
iput ASVs_counts.tsv
iput ASVs_taxonomy.tsv
```
